# 用梯度下降解决大数据集问题

标签（空格分隔）： Gradient_Descent Large_Datasets

---

###1. 简述
　　现在的学习算法比过去有效的多，其原因之一是现在拥有更多可以用来训练的数据。在处理机器学习问题时，一个有效的使算法高效的方法就是使用大量的数据训练一个低偏差的算法。但是在使用大量数据集时伴随的问题就是会有很大的计算量。
　　另外在决定使用大量数据来训练之前，可以用小部分数据（比如1000个样本）进行训练，然后绘制出学习曲线，观察算法是否是一个低偏差算法。如果算法不是低偏差的，那么可能需要一些方式让它变成低偏差的，比如增加一些特征值，或者神经网络中增加更多的隐藏层，之后再考虑收集更多的数据来进行运算。
　　
###2. 随机梯度下降（Stochastic Gradient Descent）
　　在原本的梯度下降中，以现行回归为例，假设函数、代价函数和梯度下降的执行过程如下：
　　$h_{\theta}(x) = \sum^n_{j=0}\theta_jx_j$
　　$J_{train}(\theta) = \frac 1 {2m} \sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})^2$
　　repeat {
　　　　$\theta_j := \theta_j - \alpha\frac 1 m \sum^m_{i=1}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_j$  (for every j = 0,...,n)
　　}
　　上面的算法也被称为批量梯度下降（Batch Gradient Descent）如果在训练集样本数量大的情况下，比如1亿样本，在for循环中将会执行1亿次，这将是很大的计算量。同时，由于算法的性质，每一次迭代都需要完整的读入一次全部数据，这样在运算资源有限的情况下算法会执行的相当的慢。
　　随机梯度下降可以在一定程度上解决这个问题。将上面的代价函数写成如下形式：
　　$cost(\theta, (x^{(i)}, y^{(i)})) = \frac 1 2 (h_{\theta}(x^{(i)}) - y^{(i)})^2$
　　$J_{train}(\theta) = \frac 1 m \sum^m_{i = 1}cost(\theta, (x^{(i)}, y^{(i)}))$
　　即代价函数可以表示为所有训练样本代价的和的平均值。将这个思想用在算法中，可得到随机梯度下降的算法如下：
　　- 将所有样本随机排序（可以让随机梯度下降稍微快一点收敛）
　　- repeat {
　　　　　for i:=1,...,m{
　　　　　　　$\theta_j := \theta_j - \alpha(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_j$  (for every j = 0,...,n)
　　　　　}
　　　}
　　由上述算法可以看到，随机梯度下降与批量梯度下降的主要区别在于随机梯度下降在每一次迭代时不再对所有的样本求梯度项，而仅对单个样本求梯度项。这样的话每次迭代只用获取和计算一个样本的数据，解决了上述提到的每次迭代都需要读入全部样本进行计算的问题。
　　由于算法的改变，随机梯度下降的收敛方式与批量梯度下降的收敛方式不太一样。批量梯度下降的收敛方式是基本沿着一条线向着全局最小收敛，而随机梯度下降由于每次迭代是随机的，收敛的方向也是随意变换的，但是一般情况下整体会向着全局最小值收敛的，最后可能在最小值周围的区域随机的不断的徘徊，可能无法达到最小值，但是对于一般算法来说，得到一个与最小值相近的值也已经是一个比较好的结果了。
　　另外，随机梯度下降算法中，最外层的循环次数多少一般基于样本数量，1~10是比较常见的循环次数。如果样本数量很多，可以外层循环1次。

#### 确定算法收敛
　　在之前提到的方法中，想要确定批量梯度下降是否收敛，可以根据每一次迭代计算出的参数θ计算$J_{train}(\theta)$的值，观察是否随着迭代$J_{train}(\theta)$逐渐减小。这个方法在训练集较大时运用起来比较困难，且不适用于随机梯度下降算法。
　　在随机梯度下降中确定算法是否收敛的方法如下：
　　- 使用上方的每个样本的代价：$cost(\theta, (x^{(i)}, y^{(i)})) = \frac 1 2 (h_{\theta}(x^{(i)}) - y^{(i)})^2$
　　- 在每次迭代时，在更新θ之前计算出$cost(\theta, (x^{(i)}, y^{(i)}))$的值
　　- 在比如每1000次迭代时，对过去1000个$cost(\theta, (x^{(i)}, y^{(i)}))$计算平均值，将平均值画在图像上
　　画出来的图像会有一些噪点和振荡，但是如果整体呈下降的趋势，最后趋于平稳，则可以说明算法收敛。比如每1000次迭代画出的图像可能如下图蓝色曲线，减小学习速率α后可能如红色曲线（振荡变小，可能收敛到一个更好的解），而每5000次迭代画出的曲线可能如橙色曲线（曲线更平稳，但算法表现的反馈会有一定的“延迟”）。
![stochastic_gradient_descent_convergence_1](http://97.64.17.179:8615/ml/stochastic_gradient_descent_convergence_1.png)
　　如果如下图蓝色曲线所示，图像整体趋势水平，则可能说明算法没有很好的学习，代价并没有下降；而如果用更大的样本进行平均，则可能看到如下红色曲线，呈缓慢下降趋势，则说明算法本身是在有效的学习，只是绘制蓝色曲线时用来平均的样本数量太小了，且曲线较嘈杂，看不出代价函数的趋势确实是下降的。但是如果使用更大的样本来平均依然也无法看到代价下降，则可能说明算法本身存在一些问题，需要尝试其他的方法比如调整学习速率、改变特征量等等。
![stochastic_gradient_descent_convergence_2](http://97.64.17.179:8615/ml/stochastic_gradient_descent_convergence_2.png)
　　另外如果发现图像在上升，则说明算法在发散，此时可以尝试更小的学习速率。

#### 让算法收敛到全局最小
　　上面提到，随机梯度下降在收敛后实际上是在全局最小值附近区域徘徊，而不会真的收敛到全局最小值。如果希望随机梯度下降确实收敛到全局最小值，可以随时间的变化减小学习速率α的值。比如一种典型的做法是令$\alpha = \frac {const1} {iterationNumber + const2}$，这样随着算法运行，迭代次数增加，α的值会逐渐减小，那么在收敛过程中，越接近全局最小值，振荡就会越小，直至几乎到达全局最小值。不过虽然该方法可以实现目的，但是会引入新的参数需要选择，即上述的两个常量。引入更多的参数意味着算法会更复杂，往往需要更多的精力来确定这些参数。

###3. 小批量梯度下降（Mini-Batch Gradient Descent）
　　小批量梯度下降是上述算法的一个变形，在一些情况下可能比随机梯度下降收敛的还要快。该算法与批量梯度下降和随机梯度下降的区别在于批量梯度下降每次迭代计算所有样本数量的m个样本，随机梯度下降每次迭代计算一个样本，而小批量梯度下降每次迭代计算小批量b个样本。b叫做“小批量规模”的参数，一般取值为2~100之间。
　　具体算法如下：
　　- 假设b = 10， m = 1000
　　- repeat {
　　　　　for i = 1,11,21,31,...,991 {
　　　　　　　$\theta_j := \theta_j - \alpha \frac 1 {10}\sum^{i+9}_{k=i}(h_{\theta}(x^{(k)}) - y^{(k)})x^{(k)}_j$ (for every j = 0,...,n)
　　　　　}
　　　}
　　使用小批量梯度下降的一个重要的理由是算法可以向量化。向量化实现可以一次计算b个样本的数据，这样的话可以有效利用向量化并行计算的优势加快算法的执行。
　　
###4. 在线学习机制
　　在线学习机制主要是指在线业务中有持续的数据流，可以持续的根据数据运算和学习，并直接将学习结果运用在线上业务中。对于在线学习机制来说，与之前的算法很重要的一点不同在于不再是先拥有固定的训练集，而是随着线上数据的收集，不断的获取新的样本来训练。如果能够收集的数据很多，那么在一个样本被用来训练后，可以将其丢弃，没有必要重复使用，当收集的数据不够多时再来考虑重复使用数据。在线学习机制的优势在于可以根据持续的数据流来对一些变化的内容（比如用户偏好等）进行微调和适应。

#### 例1 运输服务价格确定
　　假如有一个提供运输服务的公司，其网站在用户登录并选择出发地和目的地后给出运输价格，有的用户会接受这个价格，而有的会拒绝。根据用户的行为来优化给出的价格。
　　可能的算法：假如使用逻辑回归
　　- 用户选择的价格为正样本（y=1），拒绝的为负样本（y=0）
　　- 特征值可以选择比如出发地和目的地、此次提供给用户的价格等
　　- 学习$p(y=1|x;\theta)$来优化价格
　　- repeat forever {
　　　　　根据用户行为获取一个样本(x,y)
　　　　　用()x,y)更新θ：$\theta_j := \theta_j - \alpha(h_{\theta}(x) - y)x_j$
　　　}

#### 例2 产品搜索服务
　　假如有一个网上商城，可以搜索关键字并列出查询结果，现在想通过算法优化一下查询结果来反馈给用户更好的结果列表。
　　可能的算法：
　　- 特征值：产品的属性、查询关键字有多少与产品的属性（名称描述等等）匹配等等
　　- 用户点击的产品为正样本（y=1），否则为负样本（y=0）
　　- 学习$p(y=1|x;\theta)$ （预估点击率CTR(Click Through Rate)问题）

###5. 映射约减（Map Reduce）
　　映射约减方法是一个用来解决大规模数据运算的方法。其核心思想在于将可以并行计算的部分分配给多台计算机同时执行，之后再到中心计算机中汇总计算，最后得出结果。如下图对批量梯度下降应用映射约减：
![map_reduce](http://97.64.17.179:8615/ml/map_reduce.png)
　　上图表示将批量梯度下降中的偏导数中的求和项拆分成4份在4台计算机中并行执行，最后再将其结果合并起来。这样的话算法可以获取大约4倍速度的提升（然而实际情况因为网络延迟等原因并不能达到）。从中可以看出，如果想要使用映射约减来加速算法运行，一个关键的问题是这个需要加速的算法是否能表示为训练样本的某种求和。
　　由于现代计算机都是多核CPU，也可以将该方法运用到单个CPU的不同核心上。
