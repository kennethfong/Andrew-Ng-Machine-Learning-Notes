# Linear Regression 线性回归

tags： Linear_Regression Gradient_Descent Supervised_Learning

---
<!-- TOC -->

- [Linear Regression 线性回归](#linear-regression-线性回归)
    - [1. Cost Function 代价函数](#1-cost-function-代价函数)
    - [2. Gradient Descent 梯度下降算法](#2-gradient-descent-梯度下降算法)
    - [3. 线性回归使用梯度下降](#3-线性回归使用梯度下降)
    - [4. 梯度下降的使用技巧](#4-梯度下降的使用技巧)
        - [Feature Scaling 特征缩放](#feature-scaling-特征缩放)
        - [通过图像确定算法执行的正确性](#通过图像确定算法执行的正确性)
    - [5. 对线性回归特征的改进](#5-对线性回归特征的改进)
    - [6. Normal Equation 正规方程](#6-normal-equation-正规方程)
        - [正规方程与梯度下降算法的对比](#正规方程与梯度下降算法的对比)

<!-- /TOC -->

---

## 1. Cost Function 代价函数  
假设函数(hypothesis function)**形如**：  
$$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$$  
向量表示：令$\Theta = \begin{bmatrix}
            \theta_0 \\\\
            \vdots \\\\
            \theta_n \\\\
           \end{bmatrix}$，$
            X = \begin{bmatrix}
            x_0 \\\\
            \vdots \\\\
            x_n \\\\
           \end{bmatrix}(其中x_0 = 1)$，则有：  
$$h_\theta(x) = \Theta^TX$$  
           
代价函数：  
$$J(θ)=\frac {1}{2m}\sum_{i=1}^m(h_θ(x_i)−y_i)^2$$  
Regularization 形式：  
$$J(θ)=\frac {1}{2m}\sum_{i=1}^m(h_θ(x_i)−y_i)^2+\lambda\sum_{j=1}^n\theta_j^2$$  

Alias: Squared error function(平方误差函数) or Mean squared error(均方误差)  
意义：预测结果在对每个样本在y上的距离的和，优化的方向即是使该函数最小化。加入Regularization参数（$\lambda\sum_{j=1}^n\theta_j^2$）的目的在于防止过拟合，具体见评估学习算法的章节。  

## 2. Gradient Descent 梯度下降算法  
　　简单来说，即从某个$\Theta$开始，逐次改变$\Theta$使得$J(\theta)$的值逐渐减小，直到减小到一个可以接受的最小值（也可能仅仅只是局部最小）。其目的即是为了找到这个使$J(\theta)$最小的$\Theta$。  
　　
算法：  
    repeat until convergence {  
        　　$\theta_j := \theta_j - \alpha\frac{∂}{∂\theta_j}J(\theta_0,...,\theta_n)$  for j := 0...n  
    }  
注意，每次迭代都必须同时更新所有的$\theta$，如下的操作是**错误**的：  
$temp0 := \theta_0 - \alpha\frac{∂}{∂\theta_0}J(\theta_0,...,\theta_n)$  
$\theta_0 := temp0$  
$temp1 := \theta_1 - \alpha\frac{∂}{∂\theta_1}J(\theta_0,...,\theta_n)$  
$\theta_1 := temp1$  
$...$  

算法中的$\alpha$是指learning rate学习速率，代表着每次迭代的下降速度，太大则可能迭代太快而跨过最小值，导致无法收敛；太慢则可能导致下降太慢，会耗费很多资源。  

## 3. 线性回归使用梯度下降  
　　将线性回归的代价函数代入梯度下降算法，则有：  
    repeat until convergence {  
        　　$\theta_i := \theta_i - \alpha\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)} - y^{(i)})) · x^{(i)}$  for i := 0...n  
    }  
    
　　Regularzation形式：  
$$\begin{align*} & \text{Repeat}\ \lbrace \newline & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \rbrace \end{align*}$$  
    
    
## 4. 梯度下降的使用技巧  
### Feature Scaling 特征缩放  
　　对特征进行缩放，使所有的特征处于**相近**的数值范围内，这样梯度下降算法就能更快的收敛。这里相近的意义是相差不能太大，一般来说可能在-3到3之间，但是-1/3到1/3、-2到6也是可以接受的，-100到100、-0.0001到0.0001就不能接受了。  
方法：  
* 将需要缩放的该特征所有值除以最大值  
* 使用Mean Normalization(均值归一化)，对某特征下的所有值进行如下操作：  
$$x_i := \frac{x_i - \mu_i}{s_i}$$  
其中$\mu_i$指该特征所有值的平均数，$s_i$指该特征的最大值于最小值之差，或者标准差。  

### 通过图像确定算法执行的正确性  
　　绘制$J(\theta)$与梯度下降算法迭代次数的图像来确定算法是否正确执行。如果算法正在有效的执行，则可能绘制出的图像如下：  
![costfunction_iteration_normal](http://97.64.17.179:8615/ml/costfunction_iteration_normal.png)
其中计算θ使用的迭代次数的节点根据实际情况进行选取，当函数图像趋于平稳（$J(\theta)$减小的值小于$10^{-3}$）时可以认为已经收敛。  
　　如果看到如下的图像，则可能说明使用的learning rate α太大，可以尝试减小α：  
![costfunction_iteration_bigalpha1](http://97.64.17.179:8615/ml/costfunction_iteration_bigalpha1.png)   ![costfunction_iteration_bigalpha2](http://97.64.17.179:8615/ml/costfunction_iteration_bigalpha2.png)
　　另外如果α太小，则可能导致算法很难或者无法收敛。  

## 5. 对线性回归特征的改进  
* 根据对现实情况的判断，将多种特征值合并为1种，比如将$x_1 · x_2$作为新的特征；  
* Polynomial Regression 多项式回归，为了更好的拟合数据，采用平方、立方、平方根或者其他种类的函数来改变预测函数的形状，在拟合数据的同时也需要考虑根据猜想预测数据的趋势来共同决定使用什么样的多项式预测函数。  

## 6. Normal Equation 正规方程  
　　当前特征数为n，有m个训练样本，令$x^{(i)} = \begin{bmatrix}
            x_0^{(i)} \\\\
            \vdots \\\\
            x_n^{(i)} \\\\
           \end{bmatrix}(i \in \{1..m\})$，$X = \begin{bmatrix}
           \cdots (x^{(1)})^T \cdots \\\\
           \vdots \\\\
           \cdots (x^{(m)})^T \cdots \\\\
           \end{bmatrix}$$(X为m \times (n+1)的矩阵)$，$y =                          \begin{bmatrix}
           y^{(0)} \\\\
           \vdots \\\\
           y^{(m)} \\\\
           \end{bmatrix}$，则由下方公式可以直接计算θ的最优值：  
$$\theta = (X^TX)^{-1}X^Ty$$  
　　正规方程不需要对特征值进行特征缩放。  
　　当$X^TX$不可逆时，可能的原因是：  
* 特征值中包含冗余的特征值，这些特征值可能过于相关，这时需要删除冗余的特征值；  
* 特征值过多（比如$m \leq n$  ），这时需要删除部分特征值或者使用正规化（regularization）。  
### 正规方程与梯度下降算法的对比  
| Gradient Descent          | Normal Equation               |  
|     :----:                |     :----:                    |  
|需要选择学习速率α           |不需要选择α                     |  
|需要很多次迭代              |不需要迭代                      |  
|$O(kn^2)$                  |$O(n^3)，需要计算X^TX的逆矩阵$  |  
|当n很大时依然可以很好的运算  |当n很大时运算会变得很慢          |  
