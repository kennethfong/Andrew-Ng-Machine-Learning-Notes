# Neural Network 神经网络

标签（空格分隔）： Neural_Network Supervised_Learning

---

###1. 概述
　　神经网络算法在解决复杂的非线性分类问题上是一种比较好的算法。当特征维数很大时，在逻辑回归算法中会有很多的特征项，并且由于复杂且非线性，算法中的复杂的二次项、三次项等等会很多，可能导致算法很难有效的进行。相较而言，神经网络算法则可以较好的解决这类问题。
　　
###2. 模型描述
![neural_network](http://97.64.17.179:8615/ml/neural_network.png)
　　上图中描述了一个神经网络结构的样子。其中$a_i^{(j)}$表示第j层的第i个激励（activation），这里激励表示由一个具体的神经元读入、计算并输出的值，这里$x_1$也可以当做是$a_1^{(1)}$。另外用$\Theta^{(j)}$表示由第j层到第j+1层之间用于计算的参数矩阵。如果用$s_j$表示第j层的神经元数量，那么$\Theta^{(j)}$则是一个$s_{j+1} \times (s_j + 1)$的矩阵。另bias unit总是为1。
　　对神经网络的每一层的计算都使用相同的假设函数（hypothesis function），逐层进行计算，比如这里使用sigmod function，则：
$a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3)$
$a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3)$
$a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3)$
$h_{\Theta}(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})$
　　这个过程叫做向前传播（forward propagation）。该过程的向量计算：
　　令$z_k^{(2)} = \Theta_{k,0}^{(1)}x_0 + \Theta_{k,1}^{(1)}x_1 + \cdots + \Theta_{k,n}^{(1)}x_n$，$a^{(1)} = \begin{bmatrix} 
x_0\\ x_1\\ \vdots \\ x_n \end{bmatrix}$，则$z_k^{(2)} = \Theta_k^{(1)} a^{(1)}$；对所有的k，则有$z^{(2)} = \Theta^{(1)}a^{(1)}$，扩展至一般形式：
$$z^{(j)} = \Theta^{(j-1)}a^{(j-1)} \\
a^{(j)} = g(z^{(j)})$$

###3. 多类别分类
　　在面对多分类问题时，需要对output layer根据分类数量进行定义，比如需要将数据分成4类时，将输出结果定义为如下的形式，通过这样的定义来对输出结果进行区分：
$$y^{(i)}=\begin{bmatrix}1\\0\\0\\0\end{bmatrix},\begin{bmatrix}0\\1\\0\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\\0\end{bmatrix},\begin{bmatrix}0\\0\\0\\1\end{bmatrix}$$

###4. Cost Function
　　首先定义如下参数：
　　L -- 神经网络的总层数
　　$s_l$ -- 在第l层的神经元数（不包括偏置单元（bias unit））
　　K -- 输出层的神经元数/分类的数量
　　在使用logistic regression作为激励函数（activation function时），整个神经网络的cost function实际上是每层logistic regression的cost function之和，这里使用正则化的表达形式：
$$J(\theta)=-\frac {1}{m}\sum^m_{i=1}\sum_{k=1}^K[y_k^{(i)}log((h_\Theta(x^{(i)}))_k) + (1-y_k^{(i)})log(1-(h_\theta(x^{(i)}))_k)] + \frac \lambda {2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{j,i}^{(l)})^2$$
　　正则化部分就是各层$\Theta$除去bias unit部分的平方和。
　　
###5. Back Propagation 反向传播算法
　　使用该算法来使上述代价函数最小化。
　　记$\delta_j^{(l)}$为第l层第j个节点的误差，具体算法如下：
　　- 训练集为${(x^{(1)}, y^{(1)}), \cdots, (x^{(n)}, y^{(n)})}$
　　- 对所有的i、j、l，初始化$\Delta_{ij}^{(l)} = 0$
　　- for t = 1...m:
　　　　　1. 令$a^{(1)} := x^{(t)}$
　　　　　2. 用正向传播计算每层的$a^{(l)} \quad l=1...L$ (比如用第2部分中描述的计算方式逐层进行计算，L表示神经网络的总层数)
　　　　　3. 用当前用例的$y^{(t)}$计算误差$\delta^{(l)} = a^{(L)} - y^{(t)}$ ($a^{(L)}$即表示输出层的所有激励组成的向量)
　　　　　4. 通过该公式逐层向前计算$\delta^{(l)}$：$\delta^{(l)} = ((\Theta^{(l)})^T\delta^{(l+1)}).*a^{(l)}.*(1 - a^{(l)})$
　　　　　5. $\Delta^{(l)}_{i,j}:=\Delta^{(l)}_{i,j} + a_j^{(l)}\delta_i^{(l+1)}$，或者向量写法：$\Delta^{(l)}:=\Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$
　　- 完成对训练集的循环后计算：
　　　　$D_{i,j}^{(l)}:=\frac 1 m (\Delta_{i,j}^{(l)}+\lambda\Theta_{i,j}^{(l)})$, if j$\neq$0
　　　　$D_{i,j}^{(l)}:=\frac 1 m \Delta_{i,j}^{(l)}$, if j=0
　　其中$D_{i,j}^{(l)}$表示$J(\Theta)$对$\Theta$的偏导数，即$\frac \partial {\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{i,j}^{(l)}$，这样就可以用类似梯度下降的算法对代价函数进行计算，找到最优的θ。

####Gradient Checking
　　用于检验神经网络的模型是否正确。根据如下原理：
$$\frac \partial {\partial\Theta}J(\Theta) \approx \frac {J(\Theta + \epsilon) - J(\Theta - \epsilon)} {2\epsilon}$$
　　其中$\epsilon$是一个非常小的数，比如$\epsilon = 10^{-4}$。
　　具体算法：
```
epsilon = 1e-4;
for i = 1:n, //n表示theta中的所有元素的数量，for循环即对theta中每个元素分别用上方的方法进行计算
  thetaPlus = theta; //theta是神经网络中所有theta的集合
  thetaPlus(i) += epsilon;
  thetaMinus = theta;
  thetaMinus(i) -= epsilon;
  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)
end;
```
在最后求得gradApprox之后与上一部分求得的偏导数$DVector$进行对比，如果误差极小，比如只相差一些小数位，则基本可以认定算法没有太大问题。在验证完算法没有问题后，因为Gradient Checking本身运算速度较慢，因此取消其代码，通过算法训练出合适的theta参数。所以过程总体如下：
1. 通过反向传播算法（或者其他算法）计算偏导数$DVector$
2. 通过Gradient Checking计算出gradApprox
3. 确定gradApprox与$DVector$很接近
4. 取消Gradient Checking的代码，开始用算法对数据进行训练

####参数初始化
　　将所有theta初始化为0的方式并不适合神经网络，这会使得所有的隐藏层失去意义，故采用如下方式进行参数的初始化：
　　将所有theta参数初始化为$[-\alpha, \alpha]$之间的随机值,$\alpha$是一个任意值，比如在octave中，用如下方式进行初始化：
```
Theta1 = rand(10,11)*(2*INIT_ALPHA) - INIT_ALPHA //rand(10,11)得到的是10*11的取值在0~1之间的矩阵
```
　　神经网络层数的确定：
　　输入层：等于特征值的数量
　　输出层：等于分类的数量
　　隐藏层数：默认规则，一般使用一个隐藏层，如果使用多个隐藏层，则每个隐藏层的单元数相同。一般来说，隐藏单元数越多越好，但是越多的隐藏单元将带来越多的计算负担，一般来说每个隐藏层单元的数量取略大于输入层单元的数量是可以接受的。